{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Introduction to Neural ODEs\n",
        "subtitle: Continuous time models reading group\n",
        "date: '2022-10-07'\n",
        "author: Ke Alexander Wang\n",
        "format:\n",
        "  revealjs:\n",
        "    code-fold: true\n",
        "execute:\n",
        "  echo: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.hidden}\n",
        "$$\n",
        "% Made for MathJax\n",
        "\n",
        "\n",
        "%mathbf letters for matrices/vectors\n",
        "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\avec}{\\vec{a}}\n",
        "\\newcommand{\\bvec}{\\vec{b}}\n",
        "\\newcommand{\\cvec}{\\vec{c}}\n",
        "\\newcommand{\\dvec}{\\vec{d}}\n",
        "\\newcommand{\\evec}{\\vec{e}}\n",
        "\\newcommand{\\fvec}{\\vec{f}}\n",
        "\\newcommand{\\gvec}{\\vec{g}}\n",
        "\\newcommand{\\hvec}{\\vec{h}}\n",
        "\\newcommand{\\ivec}{\\vec{i}}\n",
        "\\newcommand{\\jvec}{\\vec{j}}\n",
        "\\newcommand{\\kvec}{\\vec{k}}\n",
        "\\newcommand{\\lvec}{\\vec{l}}\n",
        "\\newcommand{\\mvec}{\\vec{m}}\n",
        "\\newcommand{\\nvec}{\\vec{n}}\n",
        "\\newcommand{\\ovec}{\\vec{o}}\n",
        "\\newcommand{\\pvec}{\\vec{p}}\n",
        "\\newcommand{\\qvec}{\\vec{q}}\n",
        "\\newcommand{\\rvec}{\\vec{r}}\n",
        "\\newcommand{\\svec}{\\vec{s}}\n",
        "\\newcommand{\\tvec}{\\vec{t}}\n",
        "\\newcommand{\\uvec}{\\vec{u}}\n",
        "\\newcommand{\\vvec}{\\vec{v}}\n",
        "\\newcommand{\\wvec}{\\vec{w}}\n",
        "\\newcommand{\\xvec}{\\vec{x}}\n",
        "\\newcommand{\\yvec}{\\vec{y}}\n",
        "\\newcommand{\\zvec}{\\vec{z}}\n",
        "\n",
        "\\newcommand{\\abf}{\\mathbf{a}}\n",
        "\\newcommand{\\bbbf}{\\mathbf{b}}\n",
        "\\newcommand{\\cbf}{\\mathbf{c}}\n",
        "\\newcommand{\\dbf}{\\mathbf{d}}\n",
        "\\newcommand{\\ebf}{\\mathbf{e}}\n",
        "\\newcommand{\\fbf}{\\mathbf{f}}\n",
        "\\newcommand{\\gbf}{\\mathbf{g}}\n",
        "\\newcommand{\\hbf}{\\mathbf{h}}\n",
        "\\newcommand{\\ibf}{\\mathbf{i}}\n",
        "\\newcommand{\\jbf}{\\mathbf{j}}\n",
        "\\newcommand{\\kbf}{\\mathbf{k}}\n",
        "\\newcommand{\\lbf}{\\mathbf{l}}\n",
        "\\newcommand{\\mbf}{\\mathbf{m}}\n",
        "\\newcommand{\\nbf}{\\mathbf{n}}\n",
        "\\newcommand{\\obf}{\\mathbf{o}}\n",
        "\\newcommand{\\pbf}{\\mathbf{p}}\n",
        "\\newcommand{\\qbf}{\\mathbf{q}}\n",
        "\\newcommand{\\rbf}{\\mathbf{r}}\n",
        "\\newcommand{\\sbf}{\\mathbf{s}}\n",
        "\\newcommand{\\tbf}{\\mathbf{t}}\n",
        "\\newcommand{\\ubf}{\\mathbf{u}}\n",
        "\\newcommand{\\vbf}{\\mathbf{v}}\n",
        "\\newcommand{\\wbf}{\\mathbf{w}}\n",
        "\\newcommand{\\xbf}{\\mathbf{x}}\n",
        "\\newcommand{\\ybf}{\\mathbf{y}}\n",
        "\\newcommand{\\zbf}{\\mathbf{z}}\n",
        "\n",
        "\\newcommand{\\Abf}{\\mathbf{A}}\n",
        "\\newcommand{\\Bbf}{\\mathbf{B}}\n",
        "\\newcommand{\\Cbf}{\\mathbf{C}}\n",
        "\\newcommand{\\Dbf}{\\mathbf{D}}\n",
        "\\newcommand{\\Ebf}{\\mathbf{E}}\n",
        "\\newcommand{\\Fbf}{\\mathbf{F}}\n",
        "\\newcommand{\\Gbf}{\\mathbf{G}}\n",
        "\\newcommand{\\Hbf}{\\mathbf{H}}\n",
        "\\newcommand{\\Ibf}{\\mathbf{I}}\n",
        "\\newcommand{\\Jbf}{\\mathbf{J}}\n",
        "\\newcommand{\\Kbf}{\\mathbf{K}}\n",
        "\\newcommand{\\Lbf}{\\mathbf{L}}\n",
        "\\newcommand{\\Mbf}{\\mathbf{M}}\n",
        "\\newcommand{\\Nbf}{\\mathbf{N}}\n",
        "\\newcommand{\\Obf}{\\mathbf{O}}\n",
        "\\newcommand{\\Pbf}{\\mathbf{P}}\n",
        "\\newcommand{\\Qbf}{\\mathbf{Q}}\n",
        "\\newcommand{\\Rbf}{\\mathbf{R}}\n",
        "\\newcommand{\\Sbf}{\\mathbf{S}}\n",
        "\\newcommand{\\Tbf}{\\mathbf{T}}\n",
        "\\newcommand{\\Ubf}{\\mathbf{U}}\n",
        "\\newcommand{\\Vbf}{\\mathbf{V}}\n",
        "\\newcommand{\\Wbf}{\\mathbf{W}}\n",
        "\\newcommand{\\Xbf}{\\mathbf{X}}\n",
        "\\newcommand{\\Ybf}{\\mathbf{Y}}\n",
        "\\newcommand{\\Zbf}{\\mathbf{Z}}\n",
        "\n",
        "% mathsf letters\n",
        "\\newcommand{\\asf}{\\mathsf{a}}\n",
        "\\newcommand{\\bsf}{\\mathsf{b}}\n",
        "\\newcommand{\\csf}{\\mathsf{c}}\n",
        "\\newcommand{\\dsf}{\\mathsf{d}}\n",
        "\\newcommand{\\esf}{\\mathsf{e}}\n",
        "\\newcommand{\\fsf}{\\mathsf{f}}\n",
        "\\newcommand{\\gsf}{\\mathsf{g}}\n",
        "\\newcommand{\\hsf}{\\mathsf{h}}\n",
        "\\newcommand{\\isf}{\\mathsf{i}}\n",
        "\\newcommand{\\jsf}{\\mathsf{j}}\n",
        "\\newcommand{\\ksf}{\\mathsf{k}}\n",
        "\\newcommand{\\lsf}{\\mathsf{l}}\n",
        "\\newcommand{\\msf}{\\mathsf{m}}\n",
        "\\newcommand{\\nsf}{\\mathsf{n}}\n",
        "\\newcommand{\\osf}{\\mathsf{o}}\n",
        "\\newcommand{\\psf}{\\mathsf{p}}\n",
        "\\newcommand{\\qsf}{\\mathsf{q}}\n",
        "\\newcommand{\\rsf}{\\mathsf{r}}\n",
        "\\newcommand{\\ssf}{\\mathsf{s}}\n",
        "\\newcommand{\\tsf}{\\mathsf{t}}\n",
        "\\newcommand{\\usf}{\\mathsf{u}}\n",
        "\\newcommand{\\vsf}{\\mathsf{v}}\n",
        "\\newcommand{\\wsf}{\\mathsf{w}}\n",
        "\\newcommand{\\xsf}{\\mathsf{x}}\n",
        "\\newcommand{\\ysf}{\\mathsf{y}}\n",
        "\\newcommand{\\zsf}{\\mathsf{z}}\n",
        "\n",
        "\\newcommand{\\Asf}{\\mathsf{A}}\n",
        "\\newcommand{\\Bsf}{\\mathsf{B}}\n",
        "\\newcommand{\\Csf}{\\mathsf{C}}\n",
        "\\newcommand{\\Dsf}{\\mathsf{D}}\n",
        "\\newcommand{\\Esf}{\\mathsf{E}}\n",
        "\\newcommand{\\Fsf}{\\mathsf{F}}\n",
        "\\newcommand{\\Gsf}{\\mathsf{G}}\n",
        "\\newcommand{\\Hsf}{\\mathsf{H}}\n",
        "\\newcommand{\\Isf}{\\mathsf{I}}\n",
        "\\newcommand{\\Jsf}{\\mathsf{J}}\n",
        "\\newcommand{\\Ksf}{\\mathsf{K}}\n",
        "\\newcommand{\\Lsf}{\\mathsf{L}}\n",
        "\\newcommand{\\Msf}{\\mathsf{M}}\n",
        "\\newcommand{\\Nsf}{\\mathsf{N}}\n",
        "\\newcommand{\\Osf}{\\mathsf{O}}\n",
        "\\newcommand{\\Psf}{\\mathsf{P}}\n",
        "\\newcommand{\\Qsf}{\\mathsf{Q}}\n",
        "\\newcommand{\\Rsf}{\\mathsf{R}}\n",
        "\\newcommand{\\Ssf}{\\mathsf{S}}\n",
        "\\newcommand{\\Tsf}{\\mathsf{T}}\n",
        "\\newcommand{\\Usf}{\\mathsf{U}}\n",
        "\\newcommand{\\Vsf}{\\mathsf{V}}\n",
        "\\newcommand{\\Wsf}{\\mathsf{W}}\n",
        "\\newcommand{\\Xsf}{\\mathsf{X}}\n",
        "\\newcommand{\\Ysf}{\\mathsf{Y}}\n",
        "\\newcommand{\\Zsf}{\\mathsf{Z}}\n",
        "\n",
        "% mathbb letters\n",
        "\\newcommand{\\Abb}{\\mathbb{A}}\n",
        "\\newcommand{\\Cbb}{\\mathbb{C}}\n",
        "\\newcommand{\\Dbb}{\\mathbb{D}}\n",
        "\\newcommand{\\Ebb}{\\mathbb{E}}\n",
        "\\newcommand{\\Fbb}{\\mathbb{F}}\n",
        "\\newcommand{\\Gbb}{\\mathbb{G}}\n",
        "\\newcommand{\\Hbb}{\\mathbb{H}}\n",
        "\\newcommand{\\Ibb}{\\mathbb{I}}\n",
        "\\newcommand{\\Jbb}{\\mathbb{J}}\n",
        "\\newcommand{\\Kbb}{\\mathbb{K}}\n",
        "\\newcommand{\\Lbb}{\\mathbb{L}}\n",
        "\\newcommand{\\Mbb}{\\mathbb{M}}\n",
        "\\newcommand{\\Nbb}{\\mathbb{N}}\n",
        "\\newcommand{\\Obb}{\\mathbb{O}}\n",
        "\\newcommand{\\Pbb}{\\mathbb{P}}\n",
        "\\newcommand{\\Qbb}{\\mathbb{Q}}\n",
        "\\newcommand{\\Rbb}{\\mathbb{R}}\n",
        "\\newcommand{\\Sbb}{\\mathbb{S}}\n",
        "\\newcommand{\\Tbb}{\\mathbb{T}}\n",
        "\\newcommand{\\Ubb}{\\mathbb{U}}\n",
        "\\newcommand{\\Vbb}{\\mathbb{V}}\n",
        "\\newcommand{\\Wbb}{\\mathbb{W}}\n",
        "\\newcommand{\\Xbb}{\\mathbb{X}}\n",
        "\\newcommand{\\Ybb}{\\mathbb{Y}}\n",
        "\\newcommand{\\Zbb}{\\mathbb{Z}}\n",
        "\n",
        "% mathcal letters\n",
        "\\newcommand{\\Acal}{\\mathcal{A}}\n",
        "\\newcommand{\\Bcal}{\\mathcal{B}}\n",
        "\\newcommand{\\Ccal}{\\mathcal{C}}\n",
        "\\newcommand{\\Dcal}{\\mathcal{D}}\n",
        "\\newcommand{\\Ecal}{\\mathcal{E}}\n",
        "\\newcommand{\\Fcal}{\\mathcal{F}}\n",
        "\\newcommand{\\Gcal}{\\mathcal{G}}\n",
        "\\newcommand{\\Hcal}{\\mathcal{H}}\n",
        "\\newcommand{\\Ical}{\\mathcal{I}}\n",
        "\\newcommand{\\Jcal}{\\mathcal{J}}\n",
        "\\newcommand{\\Kcal}{\\mathcal{K}}\n",
        "\\newcommand{\\Lcal}{\\mathcal{L}}\n",
        "\\newcommand{\\Mcal}{\\mathcal{M}}\n",
        "\\newcommand{\\Ncal}{\\mathcal{N}}\n",
        "\\newcommand{\\Ocal}{\\mathcal{O}}\n",
        "\\newcommand{\\Pcal}{\\mathcal{P}}\n",
        "\\newcommand{\\Qcal}{\\mathcal{Q}}\n",
        "\\newcommand{\\Rcal}{\\mathcal{R}}\n",
        "\\newcommand{\\Scal}{\\mathcal{S}}\n",
        "\\newcommand{\\Tcal}{\\mathcal{T}}\n",
        "\\newcommand{\\Ucal}{\\mathcal{U}}\n",
        "\\newcommand{\\Vcal}{\\mathcal{V}}\n",
        "\\newcommand{\\Wcal}{\\mathcal{W}}\n",
        "\\newcommand{\\Xcal}{\\mathcal{X}}\n",
        "\\newcommand{\\Ycal}{\\mathcal{Y}}\n",
        "\\newcommand{\\Zcal}{\\mathcal{Z}}\n",
        "\n",
        "% shorthands\n",
        "\\newcommand{\\constant}{\\textnormal{constant}}\n",
        "% topology\n",
        "\\newcommand{\\scomp}[1]{#1^{\\mathsf c}} % set complement\n",
        "\\newcommand{\\closure}[1]{\\overline{#1}}\n",
        "\n",
        "\\newcommand{\\conv}{*}\n",
        "\\newcommand{\\circconv}{\\circledast}\n",
        "\\DeclareMathOperator{\\affinehull}{aff}\n",
        "\\DeclareMathOperator{\\convexhull}{conv}\n",
        "\\DeclareMathOperator{\\conichull}{conic}\n",
        "\\newcommand{\\pdiv}[3][]{\\operatorname{D}_{#1}\\left(#2\\:\\vert\\vert\\:#3 \\right)}\n",
        "\\newcommand{\\kldiv}[2]{\\pdiv[\\text{KL}]{#1}{#2}}\n",
        "\\newcommand{\\bigo}[1]{\\mathcal{O}\\left( #1 \\right)}\n",
        "\\newcommand{\\littleo}[1]{o\\left(#1 \\right)}\n",
        "\\newcommand{\\LHS}{\\operatorname{LHS}}\n",
        "\\newcommand{\\RHS}{\\operatorname{RHS}}\n",
        "\\newcommand{\\reals}{\\mathbb{R}}\n",
        "\\newcommand{\\posreals}{\\reals^+}\n",
        "\\newcommand{\\nonnegreals}{\\reals^{\\geq 0}}\n",
        "\\newcommand{\\naturals}{\\mathbb{N}}\n",
        "\\newcommand{\\integers}{\\mathbb{Z}}\n",
        "\\newcommand{\\complex}{\\mathbb{C}}\n",
        "\n",
        "% operators\n",
        "\\newcommand{\\textif}{\\text{if}\\ }\n",
        "\\newcommand{\\textelse}{\\text{else}\\ }\n",
        "%\\DeclareMathOperator* is used for defining operators that have limits typeset beneath them instead of to the right (at least when in a display)\n",
        "\\DeclareMathOperator*{\\maximizeop}{maximize}\n",
        "\\DeclareMathOperator*{\\minimizeop}{minimize}\n",
        "\\newcommand{\\minimize}[1]{\\minimizeop\\limits_{#1}\\ }\n",
        "\\newcommand{\\maximize}[1]{\\maximizeop\\limits_{#1}\\ }\n",
        "\\newcommand{\\subjectto}{\\text{subject to}\\ }\n",
        "\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
        "\\DeclareMathOperator*{\\argmin}{\\arg\\!\\min}\n",
        "\\DeclareMathOperator*{\\argsup}{\\arg\\!\\sup}\n",
        "\\DeclareMathOperator*{\\arginf}{\\arg\\!\\inf}\n",
        "% use \\command* to get autoscaling with paired delimiter commands\n",
        "% the unstarred version takes an optional argument that can be \\big, \\Big, etc\n",
        "% WARNING: Mathjax has a bug where there is an extra s in DeclarePairedDelimiter\n",
        "% See: https://github.com/mathjax/MathJax/issues/2758\n",
        "\\DeclarePairedDelimiters{\\ceil}{\\lceil}{\\rceil} % requires mathtools package\n",
        "\\DeclarePairedDelimiters{\\floor}{\\lfloor}{\\rfloor} % requires mathtools package\n",
        "\\DeclarePairedDelimitersX{\\inner}[2]{\\langle}{\\rangle}{#1, #2} % requires mathtools package\n",
        "\\DeclarePairedDelimiters{\\abs}{\\lvert}{\\rvert}%\n",
        "\\DeclarePairedDelimiters{\\norm}{\\lVert}{\\rVert}%\n",
        "\\DeclarePairedDelimiters{\\parens}{(}{)}\n",
        "\\DeclarePairedDelimiters{\\braks}{[}{]}\n",
        "\\newcommand{\\image}[1]{\\operatorname{img}\\parens*{#1}}\n",
        "\\newcommand{\\kernel}[1]{\\operatorname{ker}\\parens*{#1}}\n",
        "\\newcommand{\\diameter}[1]{\\operatorname{diam}\\parens*{#1}}\n",
        "\\newcommand{\\radius}[1]{\\operatorname{radius}\\parens*{#1}}\n",
        "\\newcommand{\\vecmat}[1]{\\operatorname{vec}\\parens*{#1}}\n",
        "\\newcommand{\\diag}[1]{\\operatorname{diag}\\parens*{#1}}\n",
        "\\newcommand{\\trace}[1]{\\operatorname{tr}\\parens*{#1}}\n",
        "% note: span is a latex primitive\n",
        "\\newcommand{\\vspan}[1]{\\operatorname{span}\\parens*{#1}}\n",
        "\\newcommand{\\rank}[1]{\\operatorname{rank}\\parens*{#1}}\n",
        "\\newcommand{\\sign}[1]{\\operatorname{sign}\\parens*{#1}}\n",
        "\\newcommand{\\tdef}{\\triangleq}\n",
        "% middle bar, for use in delimited expressions like sets\n",
        "\\newcommand{\\mmid}{\\ \\middle| \\ }\n",
        "%\\newcommand{\\set}[2][]{\n",
        "%    \\ifx\\\\#1\\\\\n",
        "%        \\{ #2 \\}\n",
        "%    \\else\n",
        "%        \\{ #1 \\mid  #2 \\}\n",
        "%    \\fi\n",
        "%}\n",
        "\\newcommand{\\onevec}{\\mathbf{1}}\n",
        "\\newcommand{\\neginfty}{{-\\infty}}\n",
        "% Positive semidefinite, positive definite cones\n",
        "\\newcommand{\\sym}[1]{\\Sbb^{#1}}\n",
        "\\newcommand{\\psd}[1]{\\Sbb^{#1}_+}\n",
        "\\newcommand{\\pd}[1]{\\Sbb^{#1}_{++}}\n",
        "% up/down arrows for limits\n",
        "\\newcommand{\\dto}{\\downarrow}\n",
        "\\newcommand{\\uto}{\\uparrow}\n",
        "\\newcommand{\\T}{\\top}\n",
        "\\newcommand{\\inverse}{^{-1}}\n",
        "\\newcommand{\\transpose}{{}^\\top}\n",
        "% unit ball\n",
        "\\newcommand{\\ball}{\\mathbb{B}}\n",
        "% n-sphere\n",
        "\\newcommand{\\sphere}[1]{S^{#1 - 1}}\n",
        "\n",
        "% Manifolds\n",
        "\\newcommand{\\mnf}[1]{\\manifold{#1}}\n",
        "%\\newcommand{\\manifold}[1]{\\mathcal{\\MakeUppercase{#1}}}\n",
        "\\newcommand{\\manifold}[1]{\\mathcal{#1}}\n",
        "\\newcommand{\\Mmanifold}{\\manifold{M}}\n",
        "\\newcommand{\\tangent}{\\mathrm{T}} % Tangent space\n",
        "\\newcommand{\\cotangent}{\\mathrm{T}^*} % Cotangent space\n",
        "\\newcommand{\\scalarfields}[1]{\\mathfrak{F}(#1)}\n",
        "\\newcommand{\\vectorfields}[1]{\\mathfrak{X}(#1)}\n",
        "\n",
        "% Probability\n",
        "% Random variables\n",
        "%\\newcommand{\\rv}[1]{\\mathbf{\\MakeUppercase{#1}}}\n",
        "\\newcommand{\\rv}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\arv}{\\rv{A}}\n",
        "\\newcommand{\\brv}{\\rv{B}}\n",
        "\\newcommand{\\crv}{\\rv{C}}\n",
        "\\newcommand{\\drv}{\\rv{D}}\n",
        "\\newcommand{\\erv}{\\rv{E}}\n",
        "\\newcommand{\\frv}{\\rv{F}}\n",
        "\\newcommand{\\grv}{\\rv{G}}\n",
        "\\newcommand{\\hrv}{\\rv{H}}\n",
        "\\newcommand{\\irv}{\\rv{I}}\n",
        "\\newcommand{\\jrv}{\\rv{J}}\n",
        "\\newcommand{\\krv}{\\rv{K}}\n",
        "\\newcommand{\\lrv}{\\rv{L}}\n",
        "\\newcommand{\\mrv}{\\rv{M}}\n",
        "\\newcommand{\\nrv}{\\rv{N}}\n",
        "\\newcommand{\\orv}{\\rv{O}}\n",
        "\\newcommand{\\prv}{\\rv{P}}\n",
        "\\newcommand{\\qrv}{\\rv{Q}}\n",
        "\\newcommand{\\rrv}{\\rv{R}}\n",
        "\\newcommand{\\srv}{\\rv{S}}\n",
        "\\newcommand{\\trv}{\\rv{T}}\n",
        "\\newcommand{\\urv}{\\rv{U}}\n",
        "\\newcommand{\\vrv}{\\rv{V}}\n",
        "\\newcommand{\\wrv}{\\rv{W}}\n",
        "\\newcommand{\\xrv}{\\rv{X}}\n",
        "\\newcommand{\\yrv}{\\rv{Y}}\n",
        "\\newcommand{\\zrv}{\\rv{Z}}\n",
        "\n",
        "\\newcommand{\\xspace}{\\Xcal}\n",
        "\\newcommand{\\yspace}{\\Ycal}\n",
        "\\newcommand{\\zspace}{\\Zcal}\n",
        "\n",
        "\\newcommand\\given[1][]{\\:#1\\vert\\:}\n",
        "\n",
        "\\newcommand{\\simplex}{\\Delta}\n",
        "\\newcommand{\\ev}[2][]{\\mathbb{E}_{#1}\\left[ #2\\right]}\n",
        "\\newcommand{\\var}[2][]{\\operatorname{Var}_{#1} \\braks{#2}}\n",
        "\\newcommand{\\cov}[2]{\\operatorname{Cov}\\parens*{#1, #2}}\n",
        "\\newcommand{\\prob}[2][]{\\mathbb{P}_{#1}\\left(#2\\right)}\n",
        "\\newcommand{\\ind}[1]{\\mathbbm{1}_{\\left[ #1\\right]}} % requires bbm package\n",
        "\n",
        "\\DeclareMathOperator{\\Binom}{Binom}\n",
        "\\newcommand{\\Normal}[1]{\\operatorname{Normal}\\parens*{#1}}\n",
        "\\newcommand{\\Bernoulli}[1]{\\operatorname{Bernoulli}\\parens*{#1}}\n",
        "\\newcommand{\\Binomial}[1]{\\operatorname{Binomial}\\parens*{#1}}\n",
        "\\newcommand{\\stdnormal}{\\Normal{0, 1}}\n",
        "\\newcommand{\\stdmvnormal}{\\Normal(0, I)}\n",
        "\n",
        "% convergence symbols\n",
        "\\newcommand{\\distto}{\\overset{d}{\\longrightarrow}}\n",
        "\\newcommand{\\asto}{\\overset{a.s.}{\\longrightarrow}}\n",
        "\\newcommand{\\probto}{\\overset{\\Pbb}{\\longrightarrow}}\n",
        "\\newcommand{\\lpto}[1]{\\overset{L^{#1}}{\\longrightarrow}}\n",
        "\\newcommand{\\weakto}{\\overset{\\mathrm{weak}}{\\longrightarrow}}\n",
        "\\newcommand{\\normto}{\\overset{\\mathrm{norm}}{\\longrightarrow}}\n",
        "\n",
        "% convex analysis\n",
        "\\newcommand{\\gph}[1]{\\operatorname{gph}\\parens*{#1}}\n",
        "\\newcommand{\\epigraph}[1]{\\operatorname{epi}\\parens*{#1}}\n",
        "\\newcommand{\\domain}[1]{\\operatorname{dom}\\parens*{#1}}\n",
        "\\newcommand{\\intr}[1]{\\mathrm{int}\\left( #1 \\right)}\n",
        "\\newcommand{\\clos}[1]{\\mathrm{cl}\\left( #1 \\right)}\n",
        "\\newcommand{\\relint}[1]{{#1}^{\\circ}}\n",
        "\n",
        "% groups\n",
        "\\newcommand{\\orth}[1]{\\mathsf{O}(#1)}\n",
        "\\newcommand{\\sorth}[1]{\\mathsf{SO}(#1)}\n",
        "\\newcommand{\\sunit}[1]{\\mathsf{SU}(#1)}\n",
        "\\newcommand{\\unit}[1]{\\mathsf{U}(#1)}\n",
        "\n",
        "% derivatives\n",
        "\\newcommand{\\dd}[1][]{\\mathrm{d}{#1}} % differential\n",
        "\\newcommand{\\diff}{\\textnormal{d}}\n",
        "\\newcommand{\\deriv}{\\textnormal{D}}\n",
        "\\newcommand{\\grad}{\\nabla}\n",
        "\\renewcommand{\\dfrac}[2]{\\frac{\\dd #1}{\\dd #2}}\n",
        "\\newcommand{\\pfrac}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
        "\n",
        "% matrices\n",
        "\\newcommand{\\pmat}[1]{\\begin{pmatrix} #1 \\end{pmatrix}}\n",
        "\\newcommand{\\bmat}[1]{\\begin{bmatrix} #1 \\end{bmatrix}}\n",
        "\\newcommand{\\Bmat}[1]{\\begin{Bmatrix} #1 \\end{Bmatrix}}\n",
        "$$\n",
        ":::\n",
        "\n",
        "## High-level overview of Neural ODEs:\n",
        "\n",
        "::: r-fit-text\n",
        "Neural ODEs are models that can\n",
        "\n",
        "1. learn dynamical systems specified by ODEs\n",
        "2. handle irregularly spaced time-series data\n",
        "3. be used to correct mechanistic models of dynamical systems (hybrid models)\n",
        "4. be used for generative modeling (normalizing flows)\n",
        "5. be used for latent variable models (variational autoencoders)\n",
        ":::\n",
        "\n",
        "## Review of ODEs\n",
        "::: r-fit-text\n",
        "An $n$th order ODE is one of the form:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\dd^n z}{\\dd t^n}(t) &= f(t, z(t), \\frac{\\dd z}{\\dd t}(t), \\ldots, \\frac{\\dd^{n-1}z}{\\dd t^{n-1}}(t))\n",
        "\\end{align*}\n",
        "$$\n",
        "where $z: [0, \\tau] \\to \\reals$.\n",
        "\n",
        "Think about $z(t)$ as the state of a system, and $t$ as time.\n",
        ":::\n",
        "\n",
        "## Every ODE can become a first-order ODE\n",
        "::: r-fit-text\n",
        "Regardless of its order, an ODE describes how a state $z$ evolves as a function of time $t$.\n",
        "\n",
        "Every $n$th order ODE can be written as a *system* of $n$ first order ODEs by augmenting $z$ with new variables $z_2, z_3, \\ldots, z_{n-1}$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\dd z}{\\dd t} &=: z_1 \\\\\n",
        "\\frac{\\dd z_1}{\\dd t} &=: z_2 \\\\\n",
        "&\\vdots \\\\\n",
        "\\frac{\\dd z_{n-2}}{dt} &=: z_{n-1} \\\\\n",
        "\\frac{\\dd z_{n-1}}{dt}(t) &= f(t, z(t), z_1(t), z_2(t), \\ldots, z_{n-1}(t))\n",
        "\\end{align*}\n",
        "$$\n",
        ":::\n",
        "\n",
        "## General form of an ODE (system)\n",
        "::: r-fit-text\n",
        "In general, we have a ODE of the form:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\dd z}{\\dd t}(t) &= f (t, z(t))\n",
        "\\end{align*}\n",
        "$$\n",
        "where $z(t)$ and $f(t, z(t)) \\in  \\mathbb R^D$.\n",
        "\n",
        "$f$ can be thought of as our system's \"velocity\", \n",
        "describing the instantaneous rate of change of our state \n",
        "$z$.\n",
        "\n",
        "$f$ is called the \"dynamics\" or the \"vector field\" of the ODE.\n",
        ":::\n",
        "\n",
        "## Example of a $D=2$ ODE \n",
        "\n",
        "::: r-fit-text\n",
        "Let's define an autonomous $D=2$ dynamical system where $f$ is a linear function of the state:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "f(t, z) = Az :=\n",
        "\\begin{bmatrix}\n",
        "-1/10 & -1/10 \\\\\n",
        "1/10 & -1/10\n",
        "\\end{bmatrix}\n",
        "z \n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class LinearDynamics(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        A = torch.tensor([[-1., -1],\n",
        "                          [1., -1.]]) / 10.\n",
        "        self.register_buffer(\"A\", A)\n",
        "\n",
        "    def forward(self, t, z):\n",
        "        assert t.ndim == 0\n",
        "        assert z.ndim == 2\n",
        "        return torch.mm(z, self.A.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Example of a Neural ODE\n",
        "::: r-fit-text\n",
        "Example where $f$ has learnable parameters $\\theta$, denoted as $f_\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class NeuralDynamics(torch.nn.Module):\n",
        "    def __init__(self, input_size, num_hidden=10):\n",
        "        super().__init__()\n",
        "        self.num_hidden = num_hidden\n",
        "        self.fc1 = torch.nn.Linear(input_size, num_hidden)\n",
        "        self.fc2 = torch.nn.Linear(num_hidden, input_size)\n",
        "        self.act = torch.nn.GELU()\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Initial value problems are unique\n",
        "An initial value problem consists of a differential equation and an initial condition of the function:\n",
        "$$z(0) = z_0 \\quad \\frac{\\dd z}{\\dd t}(t) = f(t, z(t)).$$\n",
        "\n",
        ":::{.callout-note}\n",
        "### Picard's Existence Theorem\n",
        "Let $f: [0, \\tau] \\times \\reals^D \\to \\reals^D$ be **continuous** in $t$ and **uniformly Lipschitz** in the second argument. Let $z_0 \\in \\reals^D$. Then there exists a unique differentiable $z: [0, \\tau] \\to \\reals^D$ satisfying\n",
        "$$z(0) = z_0 \\quad \\frac{\\dd z}{\\dd t}(t) = f(t, z(t)).$$\n",
        ":::\n",
        "\n",
        "## Integration as a forward pass\n",
        "::: r-fit-text\n",
        "Given $z(0) =: z_0$ and $f$, we can compute $z(t)$ at any $t$: \n",
        "$$\n",
        "\\begin{align*}\n",
        "z(t) &= z_0 + \\int_{0}^t f(z, t) dt\n",
        "\\end{align*}\n",
        "$$\n",
        "Not solvable analytically, instead we _numerically integrate_\n",
        "$$\n",
        "\\begin{align*}\n",
        "z(t) &\\approx \\text{ODESolver}(f, t, z_0)\n",
        "\\end{align*}\n",
        "$$\n",
        "For a given ODE solver and $f$, this can be viewed as the forward pass of a function, due to uniqueness\n",
        "$$\n",
        "z(t) = F(t, z_0)\n",
        "$$\n",
        "If $f$ has learnable parameters $\\theta$, then $F$ is learnable, and we write $f_\\theta$ and $F_\\theta$.\n",
        "We can then think of $F_\\theta$ as a function approximator (useful for classification, generative modelling)\n",
        ":::\n",
        "\n",
        "## Implementing an IVP as a forward pass\n",
        "::: r-fit-text\n",
        "We can compute the forward pass via numerical integration, assuming $T$ Euler steps, such that $t_T = t$ and $t_{i+1} = t_i + t / T$:\n",
        "$$\n",
        "\\begin{align}\n",
        "z(0) &= z_0 \\\\\n",
        "z(t_{i+1}) &= z(t_i) + f(t_i, z(t_i)) \\Delta t \\\\\n",
        "F_\\theta(t_{N}, z_0) &= z(t)\n",
        "\\end{align}\n",
        "$$\n",
        "Everything is differentiable, so we can compute gradients with respect to $t$, $z_0$, and $\\theta$!\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "\n",
        "def F(y0, f, num_steps=1000, tau=1):\n",
        "    ts = torch.linspace(0, tau, num_steps + 1)\n",
        "    dt = tau / num_steps\n",
        "    z = z0\n",
        "    for t in ts:\n",
        "        z = z + f(t, z) * dt\n",
        "    return z "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connection to residual layers\n",
        "::: r-fit-text\n",
        "The forward pass computation graph depends on:\n",
        "\n",
        "- The choice of the numerical integrator\n",
        "- Integrator parameters (like step size and duration)\n",
        "- The dynamics/vector field $f$, which can be a neural network $f_\\theta$\n",
        "\n",
        "With Euler's method, each step is a \"residual layer\" computation:\n",
        "$$z(t_{i+1}) = z(t_i) + f(t_i, z(t_i)) \\Delta t$$\n",
        "where $f_\\theta(t_i, z(t_i)) \\Delta t$ is the $i$-th residual block output.\n",
        ":::\n",
        "\n",
        "\n",
        "## Connection to SSMs\n",
        "::: r-fit-text\n",
        "An Euler discretized differential equation is the hidden state update of a state space model (SSM).\n",
        "A linear ODE with controls $u$\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\dd x}{\\dd t}(t) &= Ax(t) + Bu(t) \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "becomes\n",
        "$$\n",
        "\\begin{align}\n",
        "x_{i+1} &= x_{i} + \\Delta t Ax_i + \\Delta t B x_i \\\\\\ \n",
        " &=  \\tilde Ax_i + \\tilde Bu_i\n",
        "\\end{align}\n",
        "$$\n",
        "where we define $x_i = x(t_i)$, $u_i = u(t_i)$, $y_i = y(t_i)$, and\n",
        "$\\tilde A = (I + \\Delta t A)$, $\\tilde B = \\Delta t B$.\n",
        ":::\n",
        "\n",
        "## More general numerical integration scheme \n",
        "::: r-fit-text\n",
        "More generally, \n",
        "$$\n",
        "\\begin{align*}\n",
        "z_{i+1} &= g(f_\\theta, z_{i}, t_i)\\\\\n",
        "t_{i+1} &= t_i + h_i\n",
        "\\end{align*}\n",
        "$$\n",
        "where $h_i$ may be an adaptively chosen step size. Examples of more advanced numerical integration schemes include:\n",
        "\n",
        "- Huen's method\n",
        "- Runge-Kutta methods\n",
        "- Leapfrog method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from torchdiffeq import odeint as odeint\n",
        "\n",
        "\n",
        "class ODEModel(torch.nn.Module):\n",
        "    def __init__(self, dynamics):\n",
        "        super().__init__()\n",
        "        self.dynamics = dynamics\n",
        "        self.ode_solver = lambda f, t, z0: odeint(f, z0, t, method=\"euler\")\n",
        "\n",
        "    def forward(self, t, z0):\n",
        "        assert t.ndim == 1 \n",
        "        assert z0.ndim == 2\n",
        "        zt = self.ode_solver(self.dynamics, t, z0)\n",
        "        return zt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Visualizing numerically integrating our 2D system\n",
        "\n",
        "Let's do some numerical integration. We'll integrate `LinearDynamics` with 5 random initial conditions for 10 Euler steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "slide"
      },
      "source": [
        "t0, t1 = 0., 10.\n",
        "num_timesteps = 10\n",
        "num_trajs = 5\n",
        "state_dim = 2\n",
        "\n",
        "z0 = torch.randn(num_trajs, state_dim, device=device)\n",
        "t = torch.linspace(t0, t1, num_timesteps, device=device)\n",
        "\n",
        "linear_ode = ODEModel(LinearDynamics()).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    linear_ode_zt = linear_ode(t, z0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_traj(zt, ax=None):\n",
        "    assert zt.ndim == 3 # `num_timesteps x num_trajs x state_dim`\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    if torch.is_tensor(zt):\n",
        "        zt = zt.cpu()\n",
        "    zt_z, zt_y = zt[:, :, 0], zt[:, :, 1]\n",
        "    num_traj = zt.shape[1]\n",
        "    ax.scatter(zt_z, zt_y, color=\"k\");\n",
        "    ax.plot(zt_z, zt_y, \"y\", alpha=0.25);\n",
        "    ax.scatter(zt_z[0, :], zt_y[0, :], color=\"r\", label=r\"$z(0)$\"); # initial conditions\n",
        "    ax.scatter(zt_z[-1, :], zt_y[-1, :], color=\"b\", label=r\"$z(1)$\"); # final conditions\n",
        "    ax.set_title(f\"Trajectories from {num_traj} initial conditions\");\n",
        "    ax.axis(\"equal\")\n",
        "    ax.legend()\n",
        "    return ax\n",
        "\n",
        "_ = plot_traj(linear_ode_zt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to fit the parameters of the dynamics? \n",
        "::: r-fit-text\n",
        "If $f_\\theta$ is a parametric function, how do we fit the parameters $\\theta$?\n",
        "\n",
        "You need:\n",
        "\n",
        "1. A dataset\n",
        "2. A loss function\n",
        "3. A way to compute gradients\n",
        ":::\n",
        "\n",
        "## Ingredient: a dataset\n",
        "\n",
        "::: r-fit-text\n",
        "Let's make a dataset of trajectories with different initial conditions. Each trajectory of length $T$ looks like $z (t_0), z (t_1), \\ldots, z(t_{T-1})$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "slide"
      },
      "source": [
        "num_timesteps = 100\n",
        "num_trajs = 400\n",
        "state_dim = 2\n",
        "\n",
        "z0 = torch.randn(num_trajs, state_dim, device=device)\n",
        "t = torch.linspace(0, 10, num_timesteps, device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    zt_true = linear_ode(t, z0)\n",
        "\n",
        "def get_batch(batch_size=40, time_window=5):\n",
        "    # Create batches by choosing a random segment along a random trajectory in zt_true\n",
        "    t0 = torch.from_numpy(np.random.choice(np.arange(num_timesteps - time_window + 1, dtype=np.int64), batch_size, replace=True))\n",
        "    trajs = torch.from_numpy(np.random.choice(np.arange(num_trajs, dtype=np.int64), batch_size, replace=False))\n",
        "    batch_z0 = zt_true[t0, trajs]  # (M, D)\n",
        "    batch_t = t[:time_window]  # (T)\n",
        "    batch_zt_true = torch.stack([zt_true[t0 + dt, trajs] for dt in range(time_window)], dim=0)  # (T, M, D)\n",
        "    return batch_z0, batch_t, batch_zt_true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Ingredient: a loss function\n",
        "\n",
        "::: r-fit-text\n",
        "We can use any differentiable scalar loss function $L$ that we want, based on the task\n",
        "\n",
        "Here we use the MSE:\n",
        "$$\n",
        "\\begin{align*}\n",
        "L(z, \\hat{z}) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\| z(t_i) - \\hat{z}(t_i)\\|_2^2\n",
        "\\end{align*}\n",
        "$$\n",
        "where $z$ is the ground truth trajectory and $\\hat z$ is the predicted trajectory under the dynamics defined by $f_\\theta$.\n",
        ":::\n",
        "\n",
        "## Ingredient: a way to compute gradients\n",
        ":::r-fit-text\n",
        "\n",
        "Two main ways to compute gradients:\n",
        "\n",
        "1. Autodiff through the solver (discretize then optimize)\n",
        "    - Pros: easy to implement, fast, accurate gradients\n",
        "    - Cons: memory intensive when taking many steps \n",
        "2. Solve the continuous adjoint (optimize then discretize)\n",
        "    - Pros: memory efficient, solved in parallel with forward pass\n",
        "    - Cons: slow, inaccurate gradients\n",
        "\n",
        "We will discuss 2 later.\n",
        ":::\n",
        "\n",
        "## Let's train our own neural ODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "neural_ode = ODEModel(NeuralDynamics(state_dim)).to(device)\n",
        "\n",
        "num_iters = 100\n",
        "optimizer = torch.optim.Adam(neural_ode.parameters(), lr=1e-2)\n",
        "\n",
        "start = time.time()\n",
        "for i in range(0, num_iters + 1):\n",
        "    batch_z0, batch_t, batch_zt_true = get_batch()\n",
        "    batch_zt_pred = neural_ode(batch_t, batch_z0)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = (batch_zt_true - batch_zt_pred).pow(2).sum(-1).mean(-1).mean(-1)\n",
        "    if i % 25 == 0:\n",
        "        print(f\"Iter {i}/{num_iters} | Loss: {loss.item()}\")\n",
        "    if i > 0:\n",
        "        # 0th iteration is just to show initial loss, so we skip gradient step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        iter_time = (time.time() - start) / i\n",
        "print(f\"That took {iter_time * 1000:.2f} ms!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How did our neural ODE do?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "slide"
      },
      "source": [
        "with torch.no_grad():\n",
        "    linear_zt = linear_ode(t, z0)\n",
        "    pred_zt = neural_ode(t, z0)\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(12, 6), sharey=True)\n",
        "    \n",
        "_ = plot_traj(linear_zt, axes[0]);\n",
        "_ = axes[0].set_title(\"True trajectorys from linear dynamical system\");\n",
        "_ = axes[0].set(xlim=(-3, 3))\n",
        "_ = plot_traj(pred_zt, axes[1]);\n",
        "_ = axes[1].set_title(\"Predicted trajectorys from neural ODE\");\n",
        "_ = axes[1].set(xlim=(-3, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical tips to training a neural ODE\n",
        "::: r-fit-text\n",
        "\n",
        "1. If your GPU has enough memory, autodiff through the solver\n",
        "2. Use smooth activations like GELU\n",
        "3. Initialize $f_\\theta$ to $0$.\n",
        "4. Do not use batch norm/layer norm/etc. in $f_\\theta$\n",
        "5. Rescale inputs to $f_\\theta$ to be close to 0 and outputs to be close to the ground truth\n",
        ":::\n",
        "\n",
        "## The continuous adjoint method\n",
        "\n",
        "::: r-fit-text\n",
        "Let $L = L(z(\\tau))$.\n",
        "Let $a(t) = \\frac{\\dd L}{\\dd z(t)}$ be the adjoint variable, where $a: [0, \\tau] \\to \\reals^D$.\n",
        "Then keeping only order $\\epsilon$ terms,\n",
        "$$\n",
        "\\begin{align}\n",
        "a(t+\\epsilon)^\\top - a(t)^\\top &= a(t+\\epsilon)^\\top - \\frac{\\dd L}{\\dd z(t)} \\\\\n",
        "&= a(t+\\epsilon)^\\top - \\frac{\\dd L}{\\dd z(t+\\epsilon)} \\frac{\\dd z(t+\\epsilon)}{\\dd z(t)} \\\\\n",
        "&= a(t+\\epsilon)^\\top - a(t+\\epsilon)^\\top \\frac{\\dd}{\\dd z(t)}[z(t) + \\epsilon f_\\theta(t, z(t)) + \\ldots] \\\\ \n",
        "&= a(t+\\epsilon)^\\top - a(t+\\epsilon)^\\top [I + \\epsilon \\partial_z f_\\theta(t, z(t))] + \\ldots \\\\ \n",
        "&= -a(t)^\\top \\partial_z f_\\theta(t, z(t)) \\epsilon + \\ldots \n",
        "\\end{align}\n",
        "$$\n",
        "Hence $\\frac{\\dd a}{\\dd t}(t)^\\top = -a(t)^\\top \\partial_2 f_\\theta(t, z(t))$, with \"final condition\" $a(\\tau) = \\frac{\\dd L}{\\dd z(\\tau)}$.\n",
        ":::\n",
        "\n",
        "## Gradients from the continuous adjoint method\n",
        "::: r-fit-text\n",
        "We augment the states to get a differential equation over $[z^\\top, \\theta^\\top]^\\top$, with dynamics $[f_\\theta^\\top, 0^\\top]^\\top$.\n",
        "Then the adjoint variable is $a^\\top = [a_z^\\top, a_\\theta^\\top]$, and $\\frac{\\dd L}{\\dd \\theta} = a_\\theta(0)^\\top$.\n",
        "Plugging into prevoius results, we get an FVP we solve to get $a_\\theta(0) = \\frac{\\dd L}{\\dd \\theta}$: \n",
        "$$\n",
        "\\begin{align}\n",
        "a_z(\\tau) &= \\frac{\\dd L}{\\dd z(\\tau)} \\quad \\frac{\\dd a_z}{\\dd t}(t)^\\top = -a_z(t)^\\top \\partial_z f_\\theta (t, z(t)) \\\\\n",
        "a_\\theta(\\tau) &= 0 \\quad \\frac{\\dd a_\\theta}{\\dd t}(t)^\\top = -a_z(t)^\\top \\partial_\\theta f_\\theta (t, z(t))\n",
        "\\end{align}\n",
        "$$\n",
        ":::\n",
        "\n",
        "## Case-study: hybrid differential equations \n",
        "::: r-fit-text\n",
        "Suppose:\n",
        "\n",
        "- we know a mechanistic model $m(t, z(t))$ of a dynamical system.\n",
        "- but $g$ does not describe all the dynamics of the system: $f_{\\text{true}}(t, z(t)) = m(t, z(t)) + f_{\\text{unknown}}(t, z(t))$.\n",
        "\n",
        "Idea: we can use a neural network $f_\\theta$ to approximate $f_{\\text{unknown}}$. History goes back to the 90s!\n",
        "\n",
        "More generally, we can also add additional variables $w$ to account for unmodelled variables:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\dd z}{\\dd t} &= m(t, z(t)) + f_\\theta(t, z(t), w(t)) \\\\\n",
        "\\frac{\\dd w}{\\dd t} &= g_\\psi(t, z(t), w(t))\n",
        "\\end{align}\n",
        "$$\n",
        ":::\n",
        "\n",
        "## Case-study: density estimation with continuous normalizing flow\n",
        "::: r-fit-text\n",
        "Given:\n",
        "\n",
        "- a base random variable $\\Ubf$ with density $p_{\\Ubf}$\n",
        "- an invertible parameterized function $F_\\theta: U \\to X$,\n",
        "\n",
        "We can construct a new random variable $\\Xbf=F_\\theta(\\Ubf)$ with\n",
        "\n",
        "- a density $p_{\\theta}(x) = p_\\Ubf(F_\\theta^{-1}(x)) |\\det{\\partial_x F_\\theta^{-1}(x)}|$.\n",
        "- a way to sample from $p_\\theta$ by sampling $\\Ubf$ and applying $F_\\theta$.\n",
        "\n",
        "This is called a normalizing flow $p_\\Xbf$, trainable via maximum likelihood.\n",
        "\n",
        "Idea: we can use our IVP function $F_\\theta$ with a neural ODE $f_\\theta$. \n",
        ":::\n",
        "\n",
        "## More on continuous normalizing flow\n",
        "::: r-fit-text\n",
        "Given a data point $x\\in \\reals^D$, define final condition $z(\\tau) = x$. Usually set $\\tau=1$.\n",
        "\n",
        "Note that $\\log p_\\theta(x) = \\log p(z(1)) = \\log p_\\theta(F^{-1}_\\theta(1, x)) - \\int_1^0 \\frac{\\dd \\log p_\\theta(F^{-1}(t, x))}{\\dd t} \\dd t$.\n",
        "\n",
        "Can show that\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\dd \\log p_\\theta(z(t))}{\\dd t} &= -\\text{tr} \\frac{\\dd f_\\theta}{\\dd z(t)}\n",
        "\\end{align*}\n",
        "$$\n",
        "which is just the trace of a Jacobian, cheaply computable by Hutchinson's trace estimator\n",
        ":::\n",
        "\n",
        "## Case-study: latent variable modeling for time series\n",
        "\n",
        "::: r-fit-text\n",
        "By thinking of $F_\\theta$ as a function approximator, we can build new types of models that operate in continuous time\n",
        "\n",
        "For example:\n",
        "\n",
        "- Assume there is a latent continuous time system that is deterministic given an initial condition\n",
        "- Assume a link function that maps to the observation space\n",
        "\n",
        "This results in the forward generative model:\n",
        "$$\n",
        "\\begin{align}\n",
        "z(0) &\\sim p(z_0) \\\\\n",
        "z(t) &= z(0) + \\int_0^t f_\\theta(t, z(s)) \\dd s = F_\\theta(z(0), t) \\\\\n",
        "x(t) &\\sim p_{\\theta_x}(x(t) | z(t))\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Idea: combine this with ADVI/VAE framework to do approximate posterior inference, using an inference/encoder network $q_\\phi$, e.g. an RNN or another ODE.\n",
        ":::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}